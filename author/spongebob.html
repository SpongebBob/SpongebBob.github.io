<!DOCTYPE html>
<html lang="zh">
	<head>
		<link href="http://gmpg.org/xfn/11" rel="profile">
		<meta http-equiv="X-UA-Compatible" content="IE=edge">
		<meta http-equiv="content-type" content="text/html; charset=utf-8">

		<!-- Enable responsiveness on mobile devices-->
		<meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1">

		<title>Bobbbb - Articles by SpongeBob</title>

		<!-- CSS -->
		<link href="//fonts.googleapis.com/" rel="dns-prefetch">
		<link href="//fonts.googleapis.com/css?family=Droid+Serif:400,700,400italic|Abril+Fatface|PT+Sans:400,400italic,700&amp;subset=latin,latin-ext" rel="stylesheet">

		<link rel="stylesheet" href="/theme/css/poole.css" />
		<link rel="stylesheet" href="/theme/css/hyde.css" />
		<link rel="stylesheet" href="/theme/css/syntax.css" />
		<link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/font-awesome/4.4.0/css/font-awesome.min.css">

		<!-- RSS -->
		<link rel="alternate" type="application/rss+xml" title="RSS" href="/atom.xml">
	</head>

	<body class="theme-base-0d">
<div class="sidebar">
	<div class="container sidebar-sticky">
		<div class="sidebar-about">

			<h1>
				<a href="/">
					<img class="profile-picture" src="/images/avatar.png">
					Bobbbb
				</a>
			</h1>
			<p class="lead"></p>
			<p class="lead">I'm SpongeBob, a student from BUAA. Working in ACT lab. </p>
			<p></p>
		</div>
		<nav class="sidebar-nav">
			<a class="sidebar-nav-item" href="http://weibo.com/u/2893779987">
				<i class="fa fa-weibo"></i>
			</a>
			<a class="sidebar-nav-item" href="https://github.com/spongeBbob">
				<i class="fa fa-github"></i>
			</a>
			
		</nav>
	</div>
</div>		<div class="content container">
	<div class="posts">
			<div class="post">
				<h1 class="post-title" href="/ji-yu-yu-yi-kuo-zhan-embeddingxi-bao-ci-ku-he-svmde-wei-bo-duan-wen-ben-kuai-su-shen-du-fen-lei-ji-zhu.html#ji-yu-yu-yi-kuo-zhan-embeddingxi-bao-ci-ku-he-svmde-wei-bo-duan-wen-ben-kuai-su-shen-du-fen-lei-ji-zhu">
					<a href="/ji-yu-yu-yi-kuo-zhan-embeddingxi-bao-ci-ku-he-svmde-wei-bo-duan-wen-ben-kuai-su-shen-du-fen-lei-ji-zhu.html#ji-yu-yu-yi-kuo-zhan-embeddingxi-bao-ci-ku-he-svmde-wei-bo-duan-wen-ben-kuai-su-shen-du-fen-lei-ji-zhu">基于语义扩展Embedding细胞词库和SVM的微博短文本快速深度分类技术</a>
				</h1>
				<span class="post-date">二 06 九月 2016</span>
				<p>
					<h1>概述</h1>
<p>任务目标：给定一系列文本（微博、新闻等），做分类，分类出诸如“社会民生”，“公共安全”，“科教文卫”，“政治敏感”等</p>
<p>所用技术：布隆过滤器，支持向量机，搜狗细胞词库</p>
<ul>
<li><code>布隆过滤器</code></li>
</ul>
<p>对于原理来说很简单，位数组+k个独立hash函数。将hash函数对应的值的位数组置1，查找时如果发现所有hash函数对应位都是1说明存在，很明显这个过程并不保证查找的结果是100%正确的。</p>
<p>Bloom Filter的这种高效是有一定代价的：在判断一个元素是否属于某个集合时，有可能会把不属于这个集合的元素误认为属于这个集合（false positive）。因此，Bloom Filter不适合那些“零错误”的应用场合。而在能容忍低错误率的应用场合下，Bloom Filter通过极少的错误换取了存储空间的极大节省。</p>
<ul>
<li><code>搜狗细胞词库</code></li>
</ul>
<p>可以理解为一个词典，一系列相关的词汇，比如“传染病词库”，里面就包含了各种跟传染病相关的词，比如流行性感冒，狂犬病等等</p>
<ul>
<li><code>支持向量机</code></li>
</ul>
<p>这个展开就比较麻烦了，由于有现成的代码，我们拿他当一个分类器使用 ...</p>
				</p>
				<a class="read-more" href="ji-yu-yu-yi-kuo-zhan-embeddingxi-bao-ci-ku-he-svmde-wei-bo-duan-wen-ben-kuai-su-shen-du-fen-lei-ji-zhu.html">Continue reading »</a>
			</div>
			<div class="post">
				<h1 class="post-title" href="/incremental-learning-in-glove.html#incremental-learning-in-glove">
					<a href="/incremental-learning-in-glove.html#incremental-learning-in-glove">Incremental Learning in Glove</a>
				</h1>
				<span class="post-date">一 18 七月 2016</span>
				<p>
					<p>In industrial life, data usually become available gradually, this fact requires data analysis systems to have the capability to learn information incrementally.
Learning from new data without forgetting prior knowledge is known as incremental learning. What's more, if we don't abadon these knowledge, the stupid thing is to ...</p>
				</p>
				<a class="read-more" href="incremental-learning-in-glove.html">Continue reading »</a>
			</div>
			<div class="post">
				<h1 class="post-title" href="/bpshen-jing-wang-luo.html#bpshen-jing-wang-luo">
					<a href="/bpshen-jing-wang-luo.html#bpshen-jing-wang-luo">BP神经网络</a>
				</h1>
				<span class="post-date">一 11 七月 2016</span>
				<p>
					<p>首先我们从感知器和单个神经元的关系来讲起
单个神经元</p>
<hr />
<p>前几天上课的时候老师讲了一个感知器的例子。当时理解不深刻，只知道这东西可以通过学习来分类一个线性可分的东西。然后还讲了感知器中的一种奇怪的并不是梯度下降的方法来训练，然而梯度下降十分好理解和好用。线性分类器确实是一类比较简单的东西。</p>
<p>单个神经元就是一个感知器，加上一个<code>sigmod</code>函数即可。一个神经元的训练也可以归结到一个线性回归的问题。一个比较直观的例子来自某个slide</p>
<div class="highlight"><pre><span></span>Each day you get lunch at the cafeteria. Your diet consists of fish, chips, and ketchup. You get several portions of each. The cashier only tells you the total price of the meal. After several days ...</pre></div>
				</p>
				<a class="read-more" href="bpshen-jing-wang-luo.html">Continue reading »</a>
			</div>
			<div class="post">
				<h1 class="post-title" href="/glove-sumarry.html#glove-sumarry">
					<a href="/glove-sumarry.html#glove-sumarry">GloVe Sumarry</a>
				</h1>
				<span class="post-date">一 11 七月 2016</span>
				<p>
					<p>在前边的文章我们已经讲了词向量的两种表达方式，一种是<code>"one-hot"</code>，另外一种是<code>"distributed word representation"</code>。针对"one-hot"的种种弊端：只能表达词本身是否出现，而无法表达词与词之间的关系。而自然语言处理中的一个基本的指导思想就是，一个词的含义，跟它的上下文有关系。比如我们可以想像一下，“坚硬的”和“冰块”共同出现的概率一定要比“坚硬的”和“河流”共同出现的概率大。他们的上下文隐含一定的语义信息。</p>
<p>针使用矩阵对表示词向量的方法，主要两种方法。一种是<code>“word-doc”</code>另一种是<code>"word-word"</code>，用两种统计信息来表达一个贡献矩阵。而我们今天要讨论的<code>GloVe</code>就是采用<code>“word-word”</code>的共现矩阵。</p>
<p>但是，直接直接从共现矩阵中提取的词向量，有这样一个问题，维度非常高，和预料库的文档数，以及词汇量成正相关。并且还有另外一个问题，这个矩阵可能非常稀疏。所以会浪费很多的计算和存储资源。如何解决呢？前面已经提到了。</p>
<p>下面就是<code>"distributed ...</code></p>
				</p>
				<a class="read-more" href="glove-sumarry.html">Continue reading »</a>
			</div>
			<div class="post">
				<h1 class="post-title" href="/word-emdedding-summary.html#word-emdedding-summary">
					<a href="/word-emdedding-summary.html#word-emdedding-summary">word emdedding summary</a>
				</h1>
				<span class="post-date">日 10 七月 2016</span>
				<p>
					<p>A week ago I came to the ACT lab. My mentor gave me some papers to read. One in these papers ,<a href="http://nlp.stanford.edu/pubs/glove.pdf">GloVe: Global Vectors for Word Representation.</a></p>
<p>This paper introduces a method to  learn vector space representations of words. The model efficiently leverages statistical information by training only on the ...</p>
				</p>
				<a class="read-more" href="word-emdedding-summary.html">Continue reading »</a>
			</div>
	</div>
	<div class="pagination">
		<span class="pagination-item older">Older</span>
		<span class="pagination-item newer">Newer</span>

	</div>
		</div>
	</body>
</html>